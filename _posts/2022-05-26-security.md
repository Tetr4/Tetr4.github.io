---
layout: post
title:  Security & Pen Tests
categories:
    - Android
description: Most common Android app security findings and what to do about them.
---

The security of a product (backend, iOS and Android app) can be evaluated with penetration tests, which are often done by specialised companies. Android apps are already protected by the Android [Sandbox](https://source.android.com/security/app-sandbox) and critical operations like authentication, authorization and content storage should be done in a secure backend, so critical findings in frontend apps (that don't involve the backend) are rare. However pen test reports will usually contain a list of **common findings**, that is the same for most apps.

Management or legal might treat these common findings like a checklist, where everything must be implemented, because they are unsure of the costs and benefits and err on the side of safety. Instead advise them on a case by case basis.

In my experience the costs (complexity, bad UX, alienating users, false positives breaking the app) for these common findings are often high and potential benefits are low, because the OS leaves little room for improvements, so often **no action is necessary at all**. Not all apps are equal though. E.g. benefits for making it harder to analyze the app might match the costs in high value targets like banking apps.

Links:
- [Official Best Practices](https://developer.android.com/topic/security/best-practices)
- [SafetyNet](https://developer.android.com/training/safetynet)
- [Learnings from 5 years of tech startup code audits](https://kenkantzer.com/learnings-from-5-years-of-tech-startup-code-audits/)


# Security Tips
- Beware of [security by obscurity](https://en.wikipedia.org/wiki/Security_through_obscurity). It is incredibly easy to decompile apps or [look at their resources](https://play.google.com/store/apps/details?id=sk.styk.martin.apkanalyzer&hl=de&gl=US). It's also possible to modify the code (remove protections) and recompile it. Consider your app's code fully transparent and modifiable.
- Adversaries can run and fully analyze your app in an environment that you have no control over. Most detection heuristics are trivially circumvented. This is not an issue if you follow [security by design](https://en.wikipedia.org/wiki/Secure_by_design).
- Let the OS handle security instead of rolling your own limited solutions. The most influential thing you can do is raising the `minSdkVersion` because old OS versions ([older than 3 to 5 years](https://www.tomsguide.com/us/old-phones-unsafe,news-24846.html)) will not receive any security patches.
- Storing sensitive data in [external storage](https://developer.android.com/training/data-storage/app-specific) used to be a critical issue, which is why it [was solved on OS side](https://developer.android.com/training/data-storage/app-specific#external). The app's internal storage directory is sandboxed so it can not be accessed by other apps and is automatically encrypted since Android 10. Nevertheless it's always a good idea to **minimize persistent client side storage of sensitive data**.
- Don't roll your own crypto (algorithms), instead use the [Android crypto APIs]({% post_url 2021-10-19-crypto %}).


# Common Findings
<details markdown="1">
<summary>Clear Text Traffic</summary>

This finding is likely a false positive, as with Android 9 clear text communication (not using HTTPS) is prevented by default. False positives include deeplink URLs in manifest (so app is also opened for HTTP deeplinks) or support for local dev servers.

If you are actually using clear text communication, you should have a very good reason, e.g. exception in [security config](https://developer.android.com/training/articles/security-config) for legacy backend without maintainer.
</details>


<details markdown="1">
<summary>Tapjacking</summary>

Other apps may use overlays to listen for touch events (e.g. to get passwords).

There are two ways this can happen:
- Overlays:
    - User has [Android 6.0.1 without June security patches or Android <4.0.3](https://www.xda-developers.com/how-tapjacking-made-a-return-with-android-marshmallow-and-nobody-noticed/). This allowed apps to show a `Toast` (transparent or with misleading content) infront of other apps and intercept touch events.
    - On newer OS versions the user has to explicitly give permission for apps to draw over other apps.
    - StackOverflow solutions will recommend using [android:filterTouchesWhenObscured](https://developer.android.com/reference/kotlin/android/view/View#security). However consider raising `minSdkVersion` instead, because this is fixed on OS side, and these old versions also contain other security issues, like [Heartbleed](https://en.wikipedia.org/wiki/Heartbleed) in Android 4.1.1.
- User has a malicious keyboard app:
    - While iOS has an API for disabling third party keyboards, there is no such thing on Android.
    - Unlike iOS there is no "first party" keyboard, because every OEM can preinstall their own keyboard app.
    - Apart from that, I think users should be allowed to use their preferred keyboard.
</details>


<details markdown="1">
<summary>Secrets in Code</summary>

Findings will usually consider every secret (API keys, passwords, keystores, etc.) with the same severity. But not all secrets are equal. We have to consider these points:
- Does the secret have to be included in the app (APK or at runtime)?
- What can an adversary do with a leaked secret?
- Can we easily revoke and replace this secret?
- Is the codebase hosted in a public repository (e.g. open source project) or a private repository (propietary project)?

For public projects it makes sense to just hide all kinds of secrets in [local.properties](https://github.com/google/secrets-gradle-plugin) or environment variables and provide default secrets. This makes it easier to configure forks and prevents automated bots from grabbing them. If you pushed a secret to a public repository, consider it compromised.

For private projects it is preferable to save complexity and make building and deploying the app as easy as possible ([convention over configuration](https://en.wikipedia.org/wiki/Convention_over_configuration)). Here we can distinguish between secrets that must end up in the app and those that don't:
- Many libraries (e.g. Google Maps) store their **API keys in the manifest**. Users can just open the manifest and copy the keys. This is [by design](https://en.wikipedia.org/wiki/Secure_by_design), because there are hard restrictions on what these keys can do. As long as the key has to end up in the app at some point, e.g. if you provide these keys at runtime via a backend, it can be intercepted by a determined hacker. You can consider these kind of API keys publicly available. Having the above kind of keys **in the code** is fine for private code repositories. It's probably easier to extract the key from the app, than to gain access to the codebase.
- For other cases, check if the secret needs to appears in the app. Maybe it is only used at compile time or for an internal test variant of the app. Maybe it can be stored in (and never leave) the backend, which then acts as a **proxy to external APIs**. Especially risky general purpose secrets like AWS tokens, should not end up in the app and in the code base. Be sure to also rewrite your git commit history, when removing them.

The **keystore** with the key can be checked into private repositories. This is completely fine in my opinion for the following reasons:
- The keystore is useless without keystore password and key password (unless the passwords can be brute forced, in that case you should just move the key to keystore with a longer password). If an adversary also has access to the passwords, e.g. via secrets manager or CI server, they most likely also have access to other secrets like the keystore.
- If the keystore contains the **upload key** for [Google Play App Signing](https://developer.android.com/studio/publish/app-signing), then it would be of no use to an adversary (even if they had the passwords), as they also need access to Google Play Console (and the passwords) and you can just invalidate the upload key and generate a new one.
- If it contains the actual **signing key**, it is preferable to keep it safe in the repository, than to risk losing access to it. Adversaries still need access to Google Play Console though they might create new builds for delivery outside of Google Play if they also have access to keystore password and key password.
</details>


<details markdown="1">
<summary>Information Leakage</summary>

If you use [Logcat](https://developer.android.com/studio/command-line/logcat) for logging API requests, it may contain sensitive user data, tokens, etc. Other apps cannot access your app's logs (since Android 4.1). It can still be accesses via `adb logcat`, however this requires direct access to the user's device.

`Log.d` logs are stripped at runtime, but `Log.i`, `Log.w` and `Log.e` logs are kept. If this is an issue, make sure that these logs do not contain sensitive data or completely remove them by using these ProGuard rules (which make debugging issues in productive apps harder):

```conf
# Disable logging
-assumenosideeffects class android.util.Log {
    public static boolean isLoggable(java.lang.String, int);
    public static int v(...);
    public static int d(...);
    public static int i(...);
    public static int w(...);
    public static int e(...);
}
```
</details>


<details markdown="1">
<summary>No Root Detection</summary>

The app can react (e.g. stop working) to running on a rooted device, where the sandbox is not guaranteed.

The benefit is small:
- It can prevent this scenario: User uses your app on a rooted device, installs a malicious app and grants it root access. The malicious app can now access your app's internal data directory. With root detection the user could not use your app at all, so there is no data to access.
- Root detection is fragile and easily circumvented.

The cost is high:
- Implementing effective root detection requires a lot of effort and complexity (see tamper protection).
- About [3.6%](https://www.verimatrix.com/knowledge-base/application-security/what-is-root-detection/) of devices are rooted. Most are custom ROMs, which users install at their own risk. Some devices (One Plus, Xiaomi) are pre-rooted. Affected users can not use the app, will be frustrated and post negative reviews.

In my opinion, this is only useful for very high risk apps (banking), if at all. In that case it could be preferable to show a message to users, that their device is not safe and they are at their own risk.
</details>


<details markdown="1">
<summary>No Tamper Protection</summary>

This checks the **integrity** of app and environment, to find out if an adversary has recompiled the app or is running it in a **hostile environment** to analyze it. This usually involves:
- Root detection (see above)
- Emulator detection
- Check if debugger is connected
- â€¦

The benefit is small:
- It is marginally harder to analyze or recompile the app. Checks can be removed and circumvented.

The cost is high:
- [Implementing effective tamper protection](https://developer.android.com/training/safetynet) requires a lot of effort and complexity. It is an arms race between protection tools and bypass tools.
- This might make development harder, e.g. if the app can not run on emulators anymore. Test automation might break.

This might make sense on a [small scale](https://github.com/mukeshsolanki/Android-Tamper-Detector), that just checks the app certificate at runtime, to prevent automatic recompilation with injected malware.
</details>


<details markdown="1">
<summary>No Obfuscation</summary>

[Obfuscation with R8](https://developer.android.com/studio/build/shrink-code) makes it harder to analyze the app, but will not stop a determined hacker. You should enable it in any case, because R8 can dramatically **reduce your APK's download size** through code and resource shrinking.

You have to manage a `proguard-rules.pro` file and check your release builds for shrinking issues. If this was not done from the start of the project, it **might be too late** to get it working, because of conflicting and really hard to debug errors. This is especially hard if you use proprietary libraries, that do not provide their own `consumer-rules.pro`.

It will slow down builds, so it should only be enabled for releases:
```groovy
defaultConfig {
    ...
    proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'
}

buildTypes {
    debug {
        // set to true, if you want to debug code shrinking:
        minifyEnabled false
        shrinkResources false
    }
    release {
        minifyEnabled true
        shrinkResources true
    }
}
```
</details>


<details markdown="1">
<summary>No Certificate Pinning</summary>

Certificate pinning is [not recommended](https://developer.android.com/training/articles/security-ssl#Pinning) by Google.

The benefit is low. Communication between app and backend can not be intercepted (man-in-the-middle) in these **additional special cases**:
- One of the 150 certificate authorities is compromised and their certificate not immediately revoked by an OS update.
- Developers analyzing the app in a hostile environment (though they can just recompile the app without certificate pinning).
- Only rooted devices or devices with [Android <7](https://blog.jeroenhd.nl/article/android-7-nougat-and-certificate-authorities): Third party accessing user device's certificate store (e.g. companies installing certificates on employee devices)

The cost is high:
- Requires considerably organizational overhead, as you need to keep certificates up to date.
- Old apps will be broken and you will get negative reviews, unless the app has a force-update mechanism.
</details>
